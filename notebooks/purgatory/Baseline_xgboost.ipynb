{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    df_trans = pd.read_csv('../input/train_transaction.csv')\n",
    "    df_test_trans = pd.read_csv('../input/test_transaction.csv')\n",
    "\n",
    "    df_id = pd.read_csv('../input/train_identity.csv')\n",
    "    df_test_id = pd.read_csv('../input/test_identity.csv')\n",
    "\n",
    "    # sample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n",
    "\n",
    "    df_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True, on='TransactionID')\n",
    "    df_test = df_test_trans.merge(df_test_id, how='left', left_index=True, right_index=True, on='TransactionID')\n",
    "\n",
    "    print(df_train.shape)\n",
    "    print(df_test.shape)\n",
    "\n",
    "    # y_train = df_train['isFraud'].copy()\n",
    "    del df_trans, df_id, df_test_trans, df_test_id\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_emails(df):\n",
    "    emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n",
    "          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n",
    "          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n",
    "          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n",
    "          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n",
    "          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n",
    "          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n",
    "          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n",
    "          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n",
    "          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n",
    "          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n",
    "          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n",
    "          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n",
    "          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n",
    "          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n",
    "          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n",
    "          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n",
    "          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n",
    "          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "\n",
    "    us_emails = ['gmail', 'net', 'edu']\n",
    "\n",
    "    #https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest-579654\n",
    "    for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "        df[c + '_bin'] = df[c].map(emails)\n",
    "        \n",
    "        df[c + '_suffix'] = df[c].map(lambda x: str(x).split('.')[-1])\n",
    "        \n",
    "        df[c + '_suffix'] = df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    return df\n",
    "\n",
    "def map_transaction_amount(df):\n",
    "    df['Trans_min_mean'] = df['TransactionAmt'] - df['TransactionAmt'].mean()\n",
    "    df['Trans_min_std'] = df['Trans_min_mean'] / df['TransactionAmt'].std()\n",
    "\n",
    "    df['TransactionAmt_to_mean_card1'] = df['TransactionAmt'] / df.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "    df['TransactionAmt_to_mean_card4'] = df['TransactionAmt'] / df.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "    df['TransactionAmt_to_std_card1'] = df['TransactionAmt'] / df.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "    df['TransactionAmt_to_std_card4'] = df['TransactionAmt'] / df.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n",
    "    df['TransactionAmt_log'] = np.log(df['TransactionAmt'])\n",
    "\n",
    "    df['TransactionAmt_cents'] = df['TransactionAmt'] % 1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def encode_categorical_features(df_train, df_test):\n",
    "    # NOTE this is the only feature engineering function that takes in both df_train \n",
    "    # df_test, this is because we want values across test and train datasets\n",
    "\n",
    "    for f in df_train.drop('isFraud', axis=1).columns:\n",
    "        if df_train[f].dtype=='object' or df_test[f].dtype=='object': \n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(df_train[f].values) + list(df_test[f].values))\n",
    "            df_train[f] = lbl.transform(list(df_train[f].values))\n",
    "#             import pdb; pdb.set_trace()\n",
    "            df_test[f] = lbl.transform(list(df_test[f].values))       \n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def drop_V_features(df):\n",
    "    # applying the PCA is major PITA, for now just drop it\n",
    "    mas_v = [c for c in df.columns if c.startswith('V')]\n",
    "    df = df.drop(mas_v, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def set_X_and_y(df_train):\n",
    "    X_train = df_train.sort_values('TransactionDT').drop(['isFraud', \n",
    "                                                        'TransactionDT', \n",
    "                                                        #'Card_ID'\n",
    "                                                        ],\n",
    "                                                        axis=1)\n",
    "    y_train = df_train.sort_values('TransactionDT')['isFraud'].astype(bool)\n",
    "\n",
    "    # X_test = df_test.sort_values('TransactionDT').drop(['TransactionDT',\n",
    "    #                                                     #'Card_ID'\n",
    "    #                                                 ], \n",
    "    #                                                 axis=1)\n",
    "    # del df_train\n",
    "    # df_test = df_test[[\"TransactionDT\"]]\n",
    "\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import make_scorer\n",
    "## Hyperopt modules\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "def objective(X_train, y_train):\n",
    "    return __objective\n",
    "\n",
    "def __objective(params):\n",
    "    time1 = time.time()\n",
    "    params = {\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'gamma': \"{:.3f}\".format(params['gamma']),\n",
    "        'subsample': \"{:.2f}\".format(params['subsample']),\n",
    "        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n",
    "        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n",
    "        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n",
    "        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n",
    "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n",
    "        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n",
    "        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n",
    "    }\n",
    "\n",
    "    print(\"\\n############## New Run ################\")\n",
    "    print(f\"params = {params}\")\n",
    "    FOLDS = 7\n",
    "    count=1\n",
    "    # skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    tss = TimeSeriesSplit(n_splits=FOLDS)\n",
    "    # y_preds = np.zeros(sample_submission.shape[0])\n",
    "    # y_oof = np.zeros(X_train.shape[0])\n",
    "    score_mean = 0\n",
    "    for tr_idx, val_idx in tss.split(X_train, y_train):\n",
    "        clf = xgb.XGBClassifier(\n",
    "            n_estimators=600, random_state=4, verbose=True, \n",
    "            tree_method='hist', # my desktop does not support CUDA :( \n",
    "            # tree_method='gpu_hist',\n",
    "            # predictor='gpu_predictor',\n",
    "            nthread=8,\n",
    "            **params\n",
    "        )\n",
    "\n",
    "        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n",
    "        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        clf.fit(X_tr, y_tr)\n",
    "        #y_pred_train = clf.predict_proba(X_vl)[:,1]\n",
    "        #print(y_pred_train)\n",
    "\n",
    "        # adding needs_proba=True leads to a bad shape error\n",
    "        score = make_scorer(roc_auc_score)(clf, X_vl, y_vl)\n",
    "        # plt.show()\n",
    "        score_mean += score\n",
    "        print(f'{count} CV - score: {round(score, 4)}')\n",
    "        count += 1\n",
    "    time2 = time.time() - time1\n",
    "    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n",
    "    gc.collect()\n",
    "    print(f'Mean ROC_AUC: {score_mean / FOLDS}')\n",
    "    del X_tr, X_vl, y_tr, y_vl, clf, score\n",
    "    return -(score_mean / FOLDS)\n",
    "\n",
    "\n",
    "space = {\n",
    "    # The maximum depth of a tree, same as GBM.\n",
    "    # Used to control over-fitting as higher depth will allow model \n",
    "    # to learn relations very specific to a particular sample.\n",
    "    # Should be tuned using CV.\n",
    "    # Typical values: 3-10\n",
    "    'max_depth': hp.quniform('max_depth', 7, 23, 1),\n",
    "    \n",
    "    # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n",
    "    # (meaning pulling weights to 0). It can be more useful when the objective\n",
    "    # is logistic regression since you might need help with feature selection.\n",
    "    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n",
    "    \n",
    "    # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n",
    "    # approach can be more useful in tree-models where zeroing \n",
    "    # features might not make much sense.\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n",
    "    \n",
    "    # eta: Analogous to learning rate in GBM\n",
    "    # Makes the model more robust by shrinking the weights on each step\n",
    "    # Typical final values to be used: 0.01-0.2\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    \n",
    "    # colsample_bytree: Similar to max_features in GBM. Denotes the \n",
    "    # fraction of columns to be randomly samples for each tree.\n",
    "    # Typical values: 0.5-1\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, .9),\n",
    "    \n",
    "    # A node is split only when the resulting split gives a positive\n",
    "    # reduction in the loss function. Gamma specifies the \n",
    "    # minimum loss reduction required to make a split.\n",
    "    # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "    'gamma': hp.uniform('gamma', 0.01, .7),\n",
    "    \n",
    "    # more increases accuracy, but may lead to overfitting.\n",
    "    # num_leaves: the number of leaf nodes to use. Having a large number \n",
    "    # of leaves will improve accuracy, but will also lead to overfitting.\n",
    "    'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n",
    "    \n",
    "    # specifies the minimum samples per leaf node.\n",
    "    # the minimum number of samples (data) to group into a leaf. \n",
    "    # The parameter can greatly assist with overfitting: larger sample\n",
    "    # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n",
    "    'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n",
    "    \n",
    "    # subsample: represents a fraction of the rows (observations) to be \n",
    "    # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n",
    "    # in their paper A Scalable Tree Boosting System recommend \n",
    "    'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n",
    "    \n",
    "    # randomly select a fraction of the features.\n",
    "    # feature_fraction: controls the subsampling of features used\n",
    "    # for training (as opposed to subsampling the actual training data in \n",
    "    # the case of bagging). Smaller fractions reduce overfitting.\n",
    "    'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n",
    "    \n",
    "    # randomly bag or subsample training data.\n",
    "    'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n",
    "    \n",
    "    # bagging_fraction and bagging_freq: enables bagging (subsampling) \n",
    "    # of the training data. Both values need to be set for bagging to be used.\n",
    "    # The frequency controls how often (iteration) bagging is used. Smaller\n",
    "    # fractions and frequencies reduce overfitting.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 434)\n",
      "(506691, 433)\n",
      "Done loading data in 0.59 s\n",
      "Train: (590540, 434)\n",
      "Test: (506691, 433)\n"
     ]
    }
   ],
   "source": [
    "NUM_RUNS = 15\n",
    "\n",
    "\n",
    "# loading data takes approximately 28 secs\n",
    "time1 = time.time()\n",
    "df_train, df_test = load_data()\n",
    "time2 = time.time() - time1\n",
    "time1 = time.time()\n",
    "print(f\"Done loading data in {round(time2 / 60,2)} min\")\n",
    "print(f\"Train: {df_train.shape}\")\n",
    "print(f\"Test: {df_test.shape}\")\n",
    "\n",
    "# use only 10 000 rows for now\n",
    "df_train = df_train.sample(n=10000)\n",
    "df_test = df_test.sample(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_bak = df_train\n",
    "df_test_bak = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you fuck up\n",
    "df_train = df_train_bak\n",
    "df_test = df_test_bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_train = [c for c in df_train.columns if c.startswith('V')] # 339\n",
    "V_test = [c for c in df_test.columns if c.startswith('V')] # 339\n",
    "\n",
    "len(V_train) == len(V_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering stuff\n",
    "df_train = drop_V_features(df_train) # this should be why the training is so slow\n",
    "df_test = drop_V_features(df_test) # this should be why the training is so slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train has 95 columns\n",
      "df_test has 94 columns\n"
     ]
    }
   ],
   "source": [
    "print(f\"df_train has {df_train.shape[1]} columns\")\n",
    "print(f\"df_test has {df_test.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with feature engineering in 24.72 s\n",
      "df_train (10000, 99)\n",
      "X_train (10000, 97)\n",
      "y_train (10000,)\n",
      "                                                    \n",
      "############## New Run ################\n",
      "params = {'max_depth': 18, 'gamma': '0.438', 'subsample': '0.40', 'reg_alpha': '0.337', 'reg_lambda': '0.336', 'learning_rate': '0.040', 'num_leaves': '120.000', 'colsample_bytree': '0.602', 'min_child_samples': '170.000', 'feature_fraction': '0.451', 'bagging_fraction': '0.803'}\n",
      "  0%|          | 0/15 [00:00<?, ?it/s, best loss: ?]"
     ]
    }
   ],
   "source": [
    "assert(len(df_train.columns) == len(df_test.columns) + 1)\n",
    "\n",
    "df_train = map_emails(df_train)\n",
    "df_test = map_emails(df_test)\n",
    "\n",
    "assert(len(df_train.columns) == len(df_test.columns) + 1)\n",
    "\n",
    "df_train, df_test = encode_categorical_features(df_train, df_test)\n",
    "\n",
    "time2 = time.time() - time1\n",
    "print(f\"done with feature engineering in {round(time2 / 60, 2)} s\")\n",
    "\n",
    "X_train, y_train = set_X_and_y(df_train)\n",
    "\n",
    "print(\"df_train\", df_train.shape)\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "\n",
    "# running the optimizer\n",
    "# Set algoritm parameters\n",
    "best = fmin(fn=objective(X_train, y_train),\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=NUM_RUNS)\n",
    "\n",
    "# Print best parameters\n",
    "best_params = space_eval(space, best)\n",
    "print(\"Best_params: \", best_params)\n",
    "\n",
    "print(\"Done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TransactionID', 'TransactionDT', 'TransactionAmt', 'ProductCD',\n",
       "       'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2',\n",
       "       'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C3',\n",
       "       'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14',\n",
       "       'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11',\n",
       "       'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7',\n",
       "       'M8', 'M9', 'V1', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07',\n",
       "       'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15',\n",
       "       'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23',\n",
       "       'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31',\n",
       "       'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n",
       "       'DeviceType', 'DeviceInfo', 'P_emaildomain_bin', 'P_emaildomain_suffix',\n",
       "       'R_emaildomain_bin', 'R_emaildomain_suffix'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt',\n",
       "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
       "       'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain',\n",
       "       'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n",
       "       'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8',\n",
       "       'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4',\n",
       "       'M5', 'M6', 'M7', 'M8', 'M9', 'id_01', 'id_02', 'id_03', 'id_04',\n",
       "       'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12',\n",
       "       'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20',\n",
       "       'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28',\n",
       "       'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36',\n",
       "       'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'P_emaildomain_bin',\n",
       "       'P_emaildomain_suffix', 'R_emaildomain_bin', 'R_emaildomain_suffix'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
