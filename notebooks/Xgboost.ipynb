{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 434)\n",
      "(506691, 433)\n"
     ]
    }
   ],
   "source": [
    "df_trans = pd.read_csv('../input/train_transaction.csv')\n",
    "df_test_trans = pd.read_csv('../input/test_transaction.csv')\n",
    "\n",
    "df_id = pd.read_csv('../input/train_identity.csv')\n",
    "df_test_id = pd.read_csv('../input/test_identity.csv')\n",
    "\n",
    "sample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n",
    "\n",
    "df_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True, on='TransactionID')\n",
    "df_test = df_test_trans.merge(df_test_id, how='left', left_index=True, right_index=True, on='TransactionID')\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "# y_train = df_train['isFraud'].copy()\n",
    "del df_trans, df_id, df_test_trans, df_test_id\n",
    "\n",
    "# reduce memory usage (leads to overflow)\n",
    "# df_train = reduce_mem_usage(df_train)\n",
    "# df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy pastaed for now\n",
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n",
    "          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n",
    "          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n",
    "          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n",
    "          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n",
    "          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n",
    "          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n",
    "          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n",
    "          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n",
    "          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n",
    "          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n",
    "          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n",
    "          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n",
    "          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n",
    "          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n",
    "          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n",
    "          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n",
    "          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n",
    "          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "\n",
    "us_emails = ['gmail', 'net', 'edu']\n",
    "\n",
    "# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest-579654\n",
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    df_train[c + '_bin'] = df_train[c].map(emails)\n",
    "    df_test[c + '_bin'] = df_test[c].map(emails)\n",
    "    \n",
    "    df_train[c + '_suffix'] = df_train[c].map(lambda x: str(x).split('.')[-1])\n",
    "    df_test[c + '_suffix'] = df_test[c].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    df_train[c + '_suffix'] = df_train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    df_test[c + '_suffix'] = df_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transaction Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Trans_min_mean'] = df_train['TransactionAmt'] - df_train['TransactionAmt'].mean()\n",
    "df_train['Trans_min_std'] = df_train['Trans_min_mean'] / df_train['TransactionAmt'].std()\n",
    "df_test['Trans_min_mean'] = df_test['TransactionAmt'] - df_test['TransactionAmt'].mean()\n",
    "df_test['Trans_min_std'] = df_test['Trans_min_mean'] / df_test['TransactionAmt'].std()\n",
    "\n",
    "df_train['TransactionAmt_to_mean_card1'] = df_train['TransactionAmt'] / df_train.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "df_train['TransactionAmt_to_mean_card4'] = df_train['TransactionAmt'] / df_train.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "df_train['TransactionAmt_to_std_card1'] = df_train['TransactionAmt'] / df_train.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "df_train['TransactionAmt_to_std_card4'] = df_train['TransactionAmt'] / df_train.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n",
    "df_test['TransactionAmt_to_mean_card1'] = df_test['TransactionAmt'] / df_test.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "df_test['TransactionAmt_to_mean_card4'] = df_test['TransactionAmt'] / df_test.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "df_test['TransactionAmt_to_std_card1'] = df_test['TransactionAmt'] / df_test.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "df_test['TransactionAmt_to_std_card4'] = df_test['TransactionAmt'] / df_test.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n",
    "df_train['TransactionAmt_log'] = np.log(df_train['TransactionAmt'])\n",
    "df_test['TransactionAmt_log'] = np.log(df_test['TransactionAmt'])\n",
    "\n",
    "df_train['TransactionAmt_cents'] = df_train['TransactionAmt'] % 1\n",
    "df_test['TransactionAmt_cents'] = df_test['TransactionAmt'] % 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in df_train.drop('isFraud', axis=1).columns:\n",
    "    if df_train[f].dtype=='object' or df_test[f].dtype=='object': \n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(df_train[f].values) + list(df_test[f].values))\n",
    "        df_train[f] = lbl.transform(list(df_train[f].values))\n",
    "        df_test[f] = lbl.transform(list(df_test[f].values))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA V Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO encode categorical features\n",
    "\n",
    "# current approach does PCA on training and test data which is data snooping\n",
    "# for now just do PCA on the training set and apply that transformation on\n",
    "# the test test\n",
    "# TODO: PCA on every CV fold\n",
    "# source: https://stats.stackexchange.com/questions/55718/pca-and-the-train-test-split\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def PCA_change(df, cols, n_components, prefix=\"PCA_\", rand_seed=4):\n",
    "    # returns df (cols replaces with PCA), pca (to transform test data)\n",
    "    pca = PCA(n_components=n_components, random_state=rand_seed)\n",
    "    pca.fit(df[cols])\n",
    "    principalComponents = pca.transform(df[cols])\n",
    "\n",
    "    print(\"explained variance\", pca.explained_variance_)\n",
    "\n",
    "    principalDf = pd.DataFrame(principalComponents)\n",
    "\n",
    "    df.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "    principalDf.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n",
    "\n",
    "    df = pd.concat([df, principalDf], axis=1)\n",
    "\n",
    "    return df, pca\n",
    "\n",
    "def PCA_transform(df, cols, pca, prefix=\"PCA_\"):\n",
    "    principalComponents = pca.transform(df[cols])\n",
    "    principalDf = pd.DataFrame(principalComponents)\n",
    "\n",
    "    df.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "    principalDf.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n",
    "\n",
    "    df = pd.concat([df, principalDf], axis=1)\n",
    "\n",
    "    return df    \n",
    "\n",
    "def fillna(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].fillna((df[col].min() - 2))\n",
    "        df[col] = (minmax_scale(df[col], feature_range=(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained variance [1.33501052e+00 4.17885342e-01 2.17001711e-01 9.61400981e-02\n",
      " 5.61497797e-02 4.80469204e-02 3.77956377e-02 3.04519753e-02\n",
      " 2.66785618e-02 1.63133780e-02 7.25608269e-03 5.55054641e-03\n",
      " 4.76129330e-03 4.44079966e-03 3.95344590e-03 3.47549388e-03\n",
      " 3.31677665e-03 3.10927176e-03 2.96483588e-03 2.79745706e-03\n",
      " 2.53528055e-03 2.37490214e-03 2.11353364e-03 1.96469996e-03\n",
      " 1.87787500e-03 1.65282338e-03 1.56319475e-03 1.49009459e-03\n",
      " 1.38632359e-03 1.31973239e-03]\n",
      "Mem. usage decreased to 142.49 Mb (76.9% reduction)\n"
     ]
    }
   ],
   "source": [
    "mas_v = df_train.columns[55:394]\n",
    "\n",
    "# prepare data for PCA\n",
    "df = df_train\n",
    "fillna(df, mas_v)\n",
    "\n",
    "df, pca_train = PCA_change(df, mas_v, prefix='PCA_V_', n_components=30)\n",
    "df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 2.34137838718 (Holy shit the explained variance is very low)\n"
     ]
    }
   ],
   "source": [
    "tot_explained_var = sum([1.33501052e+00, 4.17885342e-01, 2.17001711e-01, 9.61400981e-02,\n",
    " 5.61497797e-02, 4.80469204e-02, 3.77956377e-02, 3.04519753e-02,\n",
    " 2.66785618e-02, 1.63133780e-02, 7.25608269e-03, 5.55054641e-03,\n",
    " 4.76129330e-03, 4.44079966e-03, 3.95344590e-03, 3.47549388e-03,\n",
    " 3.31677665e-03, 3.10927176e-03, 2.96483588e-03, 2.79745706e-03,\n",
    " 2.53528055e-03, 2.37490214e-03, 2.11353364e-03, 1.96469996e-03,\n",
    " 1.87787500e-03, 1.65282338e-03, 1.56319475e-03, 1.49009459e-03,\n",
    " 1.38632359e-03, 1.31973239e-03])\n",
    "\n",
    "print(f\"Explained variance: {tot_explained_var} (Holy shit the explained variance is very low)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project test data as well\n",
    "fillna(df_test, mas_v)\n",
    "df_test = PCA_transform(df_test, mas_v, pca_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionDT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18403224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18403263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18403310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18403310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18403317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionDT\n",
       "0       18403224\n",
       "1       18403263\n",
       "2       18403310\n",
       "3       18403310\n",
       "4       18403317"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df\n",
    "df_train_TransactionDT_sorted = df_train.sort_values('TransactionDT')\n",
    "X_train = df_train_TransactionDT_sorted.drop(['isFraud', 'TransactionDT', 'TransactionID'], axis=1)\n",
    "y_train = df_train_TransactionDT_sorted['isFraud'].astype(bool)\n",
    "\n",
    "X_test = df_test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], \n",
    "                                                   axis=1)\n",
    "del df_train\n",
    "df_test = df_test[[\"TransactionDT\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train 134\n",
      "X_test 134\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train\", len(X_train.columns))\n",
    "print(\"X_test\", len(X_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionDT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18403224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18403263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18403310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18403310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18403317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18403323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18403350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18403387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18403405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18403416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18403474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18403504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18403508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18403543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18403553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18403582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18403598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18403632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18403632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18403678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>18403686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>18403735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>18403755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18403788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>18403829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>18403844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18403848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18403854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>18403857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>18403863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506661</th>\n",
       "      <td>34213723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506662</th>\n",
       "      <td>34213751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506663</th>\n",
       "      <td>34213763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506664</th>\n",
       "      <td>34213767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506665</th>\n",
       "      <td>34213780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506666</th>\n",
       "      <td>34213798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506667</th>\n",
       "      <td>34213812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506668</th>\n",
       "      <td>34213826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506669</th>\n",
       "      <td>34213834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506670</th>\n",
       "      <td>34213938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506671</th>\n",
       "      <td>34213992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506672</th>\n",
       "      <td>34214060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506673</th>\n",
       "      <td>34214073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506674</th>\n",
       "      <td>34214101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506675</th>\n",
       "      <td>34214125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506676</th>\n",
       "      <td>34214138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506677</th>\n",
       "      <td>34214147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506678</th>\n",
       "      <td>34214153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506679</th>\n",
       "      <td>34214214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506680</th>\n",
       "      <td>34214216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506681</th>\n",
       "      <td>34214253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506682</th>\n",
       "      <td>34214261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506683</th>\n",
       "      <td>34214269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506684</th>\n",
       "      <td>34214271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506685</th>\n",
       "      <td>34214277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506686</th>\n",
       "      <td>34214279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506687</th>\n",
       "      <td>34214287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506688</th>\n",
       "      <td>34214326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506689</th>\n",
       "      <td>34214337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506690</th>\n",
       "      <td>34214345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506691 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TransactionDT\n",
       "0            18403224\n",
       "1            18403263\n",
       "2            18403310\n",
       "3            18403310\n",
       "4            18403317\n",
       "5            18403323\n",
       "6            18403350\n",
       "7            18403387\n",
       "8            18403405\n",
       "9            18403416\n",
       "10           18403474\n",
       "11           18403504\n",
       "12           18403508\n",
       "13           18403543\n",
       "14           18403553\n",
       "15           18403582\n",
       "16           18403598\n",
       "17           18403632\n",
       "18           18403632\n",
       "19           18403678\n",
       "20           18403686\n",
       "21           18403735\n",
       "22           18403755\n",
       "23           18403788\n",
       "24           18403829\n",
       "25           18403844\n",
       "26           18403848\n",
       "27           18403854\n",
       "28           18403857\n",
       "29           18403863\n",
       "...               ...\n",
       "506661       34213723\n",
       "506662       34213751\n",
       "506663       34213763\n",
       "506664       34213767\n",
       "506665       34213780\n",
       "506666       34213798\n",
       "506667       34213812\n",
       "506668       34213826\n",
       "506669       34213834\n",
       "506670       34213938\n",
       "506671       34213992\n",
       "506672       34214060\n",
       "506673       34214073\n",
       "506674       34214101\n",
       "506675       34214125\n",
       "506676       34214138\n",
       "506677       34214147\n",
       "506678       34214153\n",
       "506679       34214214\n",
       "506680       34214216\n",
       "506681       34214253\n",
       "506682       34214261\n",
       "506683       34214269\n",
       "506684       34214271\n",
       "506685       34214277\n",
       "506686       34214279\n",
       "506687       34214287\n",
       "506688       34214326\n",
       "506689       34214337\n",
       "506690       34214345\n",
       "\n",
       "[506691 rows x 1 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the HyperOpt function with parameters space and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperopt modules\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.model_selection import KFold,TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import time\n",
    "def objective(params):\n",
    "    time1 = time.time()\n",
    "    params = {\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'gamma': \"{:.3f}\".format(params['gamma']),\n",
    "        'subsample': \"{:.2f}\".format(params['subsample']),\n",
    "        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n",
    "        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n",
    "        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n",
    "        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n",
    "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n",
    "        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n",
    "        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n",
    "    }\n",
    "\n",
    "    print(\"\\n############## New Run ################\")\n",
    "    print(f\"params = {params}\")\n",
    "    FOLDS = 7\n",
    "    count=1\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    tss = TimeSeriesSplit(n_splits=FOLDS)\n",
    "    y_preds = np.zeros(sample_submission.shape[0])\n",
    "    y_oof = np.zeros(X_train.shape[0])\n",
    "    score_mean = 0\n",
    "    for tr_idx, val_idx in tss.split(X_train, y_train):\n",
    "        clf = xgb.XGBClassifier(\n",
    "            n_estimators=600, random_state=4, verbose=True, \n",
    "            tree_method='gpu_hist', \n",
    "            **params\n",
    "        )\n",
    "\n",
    "        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n",
    "        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        clf.fit(X_tr, y_tr)\n",
    "        #y_pred_train = clf.predict_proba(X_vl)[:,1]\n",
    "        #print(y_pred_train)\n",
    "        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n",
    "        # plt.show()\n",
    "        score_mean += score\n",
    "        print(f'{count} CV - score: {round(score, 4)}')\n",
    "        count += 1\n",
    "    time2 = time.time() - time1\n",
    "    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n",
    "    gc.collect()\n",
    "    print(f'Mean ROC_AUC: {score_mean / FOLDS}')\n",
    "    del X_tr, X_vl, y_tr, y_vl, clf, score\n",
    "    return -(score_mean / FOLDS)\n",
    "\n",
    "\n",
    "space = {\n",
    "    # The maximum depth of a tree, same as GBM.\n",
    "    # Used to control over-fitting as higher depth will allow model \n",
    "    # to learn relations very specific to a particular sample.\n",
    "    # Should be tuned using CV.\n",
    "    # Typical values: 3-10\n",
    "    'max_depth': hp.quniform('max_depth', 7, 23, 1),\n",
    "    \n",
    "    # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n",
    "    # (meaning pulling weights to 0). It can be more useful when the objective\n",
    "    # is logistic regression since you might need help with feature selection.\n",
    "    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n",
    "    \n",
    "    # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n",
    "    # approach can be more useful in tree-models where zeroing \n",
    "    # features might not make much sense.\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n",
    "    \n",
    "    # eta: Analogous to learning rate in GBM\n",
    "    # Makes the model more robust by shrinking the weights on each step\n",
    "    # Typical final values to be used: 0.01-0.2\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    \n",
    "    # colsample_bytree: Similar to max_features in GBM. Denotes the \n",
    "    # fraction of columns to be randomly samples for each tree.\n",
    "    # Typical values: 0.5-1\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, .9),\n",
    "    \n",
    "    # A node is split only when the resulting split gives a positive\n",
    "    # reduction in the loss function. Gamma specifies the \n",
    "    # minimum loss reduction required to make a split.\n",
    "    # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "    'gamma': hp.uniform('gamma', 0.01, .7),\n",
    "    \n",
    "    # more increases accuracy, but may lead to overfitting.\n",
    "    # num_leaves: the number of leaf nodes to use. Having a large number \n",
    "    # of leaves will improve accuracy, but will also lead to overfitting.\n",
    "    'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n",
    "    \n",
    "    # specifies the minimum samples per leaf node.\n",
    "    # the minimum number of samples (data) to group into a leaf. \n",
    "    # The parameter can greatly assist with overfitting: larger sample\n",
    "    # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n",
    "    'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n",
    "    \n",
    "    # subsample: represents a fraction of the rows (observations) to be \n",
    "    # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n",
    "    # in their paper A Scalable Tree Boosting System recommend \n",
    "    'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n",
    "    \n",
    "    # randomly select a fraction of the features.\n",
    "    # feature_fraction: controls the subsampling of features used\n",
    "    # for training (as opposed to subsampling the actual training data in \n",
    "    # the case of bagging). Smaller fractions reduce overfitting.\n",
    "    'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n",
    "    \n",
    "    # randomly bag or subsample training data.\n",
    "    'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n",
    "    \n",
    "    # bagging_fraction and bagging_freq: enables bagging (subsampling) \n",
    "    # of the training data. Both values need to be set for bagging to be used.\n",
    "    # The frequency controls how often (iteration) bagging is used. Smaller\n",
    "    # fractions and frequencies reduce overfitting.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "############## New Run ################\n",
      "params = {'max_depth': 13, 'gamma': '0.515', 'subsample': '0.60', 'reg_alpha': '0.035', 'reg_lambda': '0.059', 'learning_rate': '0.030', 'num_leaves': '220.000', 'colsample_bytree': '0.676', 'min_child_samples': '150.000', 'feature_fraction': '0.648', 'bagging_fraction': '0.877'}\n",
      "  0%|          | 0/27 [00:00<?, ?it/s, best loss: ?]"
     ]
    }
   ],
   "source": [
    "# Set algoritm parameters\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=27)\n",
    "\n",
    "# Print best parameters\n",
    "best_params = space_eval(space, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
